{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Libaries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "# Import Os to get to the root directory\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "TEST_DIR = os.path.join(ROOT_DIR, \"test-white-papers\")\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"model\")\n",
    "\n",
    "# First, we define the path where our Train dataset is located\n",
    "TRAIN_DIR = os.path.join(ROOT_DIR, \"dataset\")\n",
    "TRAIN_CSV = os.path.join(TRAIN_DIR, \"train.csv\")\n",
    "        \n",
    "# Import own functions\n",
    "from libraries import corpus\n",
    "from libraries import pdf2text\n",
    "\n",
    "##########################\n",
    "# Import other libraries\n",
    "##########################\n",
    "\n",
    "# Data Processing and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Methods Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        # Retrieving the PDF files from white-paper folder and converting them to text\n",
    "        self.dataset = pdf2text.get_dataset()\n",
    "        \n",
    "        # Define if you want your corpus to be whole or divided in sentences\n",
    "        makeSentences = True\n",
    "        # Creates a dictionary with each White Paper and its text pre-processed\n",
    "        self.corpora_sent = corpus.makeCleanCorpus(dataset, lemmatize=True, removePunct=True, removeNums=True\n",
    "                            ,makeSentences=makeSentences, removeURL=True, removeChar=True, removeEnt=True)\n",
    "        \n",
    "        # Define if you want your corpus to be whole or divided in sentences\n",
    "        makeSentences = False\n",
    "        # Creates a dictionary with each White Paper and its text pre-processed\n",
    "        self.corpora_whole = corpus.makeCleanCorpus(dataset, lemmatize=True, removePunct=True, removeNums=True\n",
    "                            ,makeSentences=makeSentences, removeURL=True, removeChar=True, removeEnt=True)\n",
    "    \n",
    "        # Retrieve the NEW White Papers to Test\n",
    "        self.testset = pdf2text.get_dataset(path=TEST_DIR)\n",
    "        \n",
    "        # Define if you want your corpus to be whole or divided in sentences\n",
    "        makeSentences = True\n",
    "        # Creates a dictionary with each White Paper and its text pre-processed\n",
    "        self.corpora_test_sent = corpus.makeCleanCorpus(test, lemmatize=True, removePunct=True, removeNums=True\n",
    "                                ,makeSentences=makeSentences, removeURL=True, removeChar=True, removeEnt=True)\n",
    "        \n",
    "        # Define if you want your corpus to be whole or divided in sentences\n",
    "        makeSentences = False\n",
    "        \n",
    "        # Creates a dictionary with each White Paper and its text pre-processed\n",
    "        self.corpora_test_whole = corpus.makeCleanCorpus(test, lemmatize=True, removePunct=True, removeNums=True\n",
    "                                 ,makeSentences=makeSentences, removeURL=True, removeChar=True, removeEnt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformationRetrieval():\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    # Define function\n",
    "    def prepare_data(self, corpus, protocol):\n",
    "        \"\"\"\n",
    "        Function that receives a name of a protocol and return the tokens.\n",
    "        Input:\n",
    "            Name of protocol\n",
    "        Output:\n",
    "            Tokens\n",
    "        \"\"\"\n",
    "        assert protocol in corpus.keys()\n",
    "\n",
    "        # Start tokenizing the dataset\n",
    "        tokenizer = nltk.tokenize.RegexpTokenizer('\\s+', gaps=True)\n",
    "\n",
    "        tokens = tokenizer.tokenize(corpus[protocol])\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    def visualize_ngrams(self,tokens, n = 2):\n",
    "    \n",
    "        assert n > 1\n",
    "\n",
    "        if n == 2:\n",
    "            bigrams_tokens = bigrams(tokens)\n",
    "            fdist_bigrams = FreqDist(list(bigrams_tokens))\n",
    "            fdist_bigrams.plot(30,cumulative=False)\n",
    "            return plt\n",
    "        if n == 3:\n",
    "            trigrams_tokens = ngrams(tokens, 3)\n",
    "            fdist_trigrams = FreqDist(list(trigrams_tokens))\n",
    "            fdist_trigrams.plot(30,cumulative=False)\n",
    "            return plt\n",
    "        if n > 3:\n",
    "            grams_tokens = ngrams(tokens, n)\n",
    "            fdist_grams = FreqDist(list(grams_tokens))\n",
    "            fdist_grams.plot(30,cumulative=False)\n",
    "            return plt\n",
    "            \n",
    "    \n",
    "    def visualize_word_cloud(self,corpus,protocol):\n",
    "        text = corpus[protocol]\n",
    "        wordcloud = WordCloud(background_color=\"white\",\n",
    "                              stopwords = set(STOPWORDS)\n",
    "                              ).generate(text)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentClustering():\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        # Function that converts a dictionary into a Pandas Dataframe\n",
    "        # The indexes are the name of the files\n",
    "        self.df = corpus.dictionaryToPandas(self.data.corpora_whole)\n",
    "\n",
    "        # Instantiate TfidfVectorizer object with stopwords and tokenizer\n",
    "        # parameters for efficient processing of text\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=20000,\n",
    "                                                stop_words='english', lowercase = True,\n",
    "                                                 use_idf=True, tokenizer=self.tokenize_and_stem,\n",
    "                                                 ngram_range=(1,3))\n",
    "\n",
    "        # Fit and transform the tfidf_vectorizer with the \"text\" of each paper\n",
    "        # to create a vector representation of the plot summaries\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(self.data.corpora_whole.values())\n",
    "\n",
    "        self.matrix = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names(),index=self.data.corpora_whole.keys())\n",
    "\n",
    "    # Define a function to perform both stemming and tokenization\n",
    "    def tokenize_and_stem(self,text):\n",
    "\n",
    "        # Tokenize by sentence, then by word\n",
    "        tokens = [word for text in nltk.sent_tokenize(text) for word in nltk.word_tokenize(text)]\n",
    "\n",
    "        # Filter out raw tokens to remove noise\n",
    "        filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n",
    "\n",
    "        # Stem the filtered_tokens\n",
    "        stems = []\n",
    "        for token in filtered_tokens:\n",
    "             stems.append(stemmer.stem(token))\n",
    "\n",
    "        return filtered_tokens\n",
    "    \n",
    "    # Defining Function\n",
    "    def clustering_on_wordvecs(self,word_vectors, num_clusters):\n",
    "        \"\"\"\n",
    "        Function that receives a word vectors and a number of clusters, and return the centers and the clusters\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initalize a k-means object and use it to extract centroids\n",
    "        kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++');\n",
    "        \n",
    "        idx = kmeans_clustering.fit_predict(word_vectors);\n",
    "\n",
    "        return kmeans_clustering.cluster_centers_, idx;\n",
    "\n",
    "    #Average out vectors in a document\n",
    "    def average_word_vectors(self,words, model, vocabulary, num_features):\n",
    "        \"\"\"\n",
    "        Function that receives a words, a model, a vocabulary and number of features and return averages\n",
    "\n",
    "        \"\"\"\n",
    "        feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "\n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model[word])\n",
    "\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "    #Process a single document \n",
    "    def averaged_word_vectorizer(self,corpus, model, num_features):\n",
    "        vocabulary = set(model.wv.index2word)\n",
    "        features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                        for tokenized_sentence in corpus]\n",
    "        \n",
    "        return np.array(features)\n",
    "\n",
    "    \n",
    "    def clean_text(self,all_documents):\n",
    "        rslt = []\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle');\n",
    "        for pos in range(len(all_documents)):\n",
    "            doc_rslt = []\n",
    "            # Get the comment\n",
    "            doc = all_documents[pos];\n",
    "\n",
    "            # Normalize tabs and remove newlines\n",
    "            no_tabs = str(doc).replace('\\t', ' ').replace('\\n', '');\n",
    "\n",
    "            # Remove all characters except A-Z and a dot.\n",
    "            alphas_only = re.sub(\"[^a-zA-Z\\.]\", \" \", no_tabs);\n",
    "\n",
    "            # Normalize spaces to 1\n",
    "            multi_spaces = re.sub(\" +\", \" \", alphas_only);\n",
    "\n",
    "            # Strip trailing and leading spaces\n",
    "            no_spaces = multi_spaces.strip();\n",
    "\n",
    "            # Normalize all charachters to lowercase\n",
    "            clean_text = no_spaces.lower();\n",
    "\n",
    "            # Get sentences from the tokenizer, remove the dot in each.\n",
    "            sentences = tokenizer.tokenize(clean_text);\n",
    "            sentences = [re.sub(\"[\\.]\", \"\", sentence) for sentence in sentences];\n",
    "\n",
    "            # If the text has more than one space (removing single word comments) and one character, write it to the file.\n",
    "            if len(clean_text) > 0 and clean_text.count(' ') > 0:\n",
    "                for sentence in sentences:\n",
    "                    for word in sentence.split(\" \"):\n",
    "                        doc_rslt.append(word)\n",
    "\n",
    "            rslt.append(doc_rslt)\n",
    "\n",
    "        return rslt\n",
    "\n",
    "    def get_word2vec_model(self):\n",
    "        \n",
    "        # Load the model\n",
    "        self.model = word2vec.Word2Vec.load(\"model_word2vec_clustering.model\")\n",
    "\n",
    "    def get_ap_model(self):\n",
    "        \n",
    "        #COnverting into list of words\n",
    "        cleaned_text = self.clean_text(list(self.data.corpora_whole.values()))\n",
    "\n",
    "        w2v_feature_array = self.averaged_word_vectorizer(corpus=cleaned_text, model=self.model,\n",
    "                                                     num_features=100)\n",
    "        # Create an\n",
    "        self.ap = AffinityPropagation()\n",
    "\n",
    "        self.ap.fit(w2v_feature_array)\n",
    "\n",
    "        ap_centers = ap.cluster_centers_\n",
    "\n",
    "        cluster_labels = ap.labels_\n",
    "        cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "\n",
    "        self.df = self.df.reset_index()\n",
    "        rslt_df = pd.concat([self.df, cluster_labels], axis=1)\n",
    "\n",
    "        clus_val = rslt_df.ClusterLabel.value_counts()\n",
    "\n",
    "        # Create a Dataframe with the predicted clusters\n",
    "        self.reference = rslt_df[[\"index\",\"cluster\",\"ClusterLabel\"]]\n",
    "\n",
    "        self.reference.columns = [\"protocol\",\"kmeans_cluster\",\"ap_cluster\"]\n",
    "\n",
    "\n",
    "    def visualize_ap_clusters(self,protocol):\n",
    "        # Application of Affinity Propagation Cluster to a specific white paper\n",
    "        cleaned_text = self.clean_text(list([self.data.corpora_test_whole[protocol]]))\n",
    "\n",
    "        w2v_feature_array = averaged_word_vectorizer(corpus=cleaned_text, model=self.model,\n",
    "                                                     num_features=100)\n",
    "\n",
    "        predict = self.ap.predict(w2v_feature_array)\n",
    "\n",
    "        # List of other protocols in the same cluster\n",
    "        similars = self.reference[self.reference.ap_cluster == predict[0]].protocol\n",
    "        \n",
    "        stats = corpus.get_datastats()\n",
    "\n",
    "        result = pd.merge(similars.to_frame(),stats, left_on=\"protocol\",right_on=\"name\",how=\"inner\")\n",
    "\n",
    "        result = result.sort_values(by=\"rank\")\n",
    "\n",
    "        plt.plot(result.protocol, result[\"rank\"])\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.xlabel(\"Protocols in Cluster\")\n",
    "        plt.ylabel(\"Rank\")\n",
    "        \n",
    "        return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextExtraction():\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \n",
    "        # Prepare the array with the tokens\n",
    "        self.tokens = []\n",
    "\n",
    "        # Iterate through all the papers and get all the different words within the papers\n",
    "        for k in self.data.corpora_whole.keys():\n",
    "            self.tokens.append(word_tokenize(corpora[k]))\n",
    "    \n",
    "    def get_word2vec_model(self):\n",
    "        \n",
    "        self.model = Word2Vec.load(\"model_word2vec_textextraction.model\")\n",
    "    \n",
    "    # Define Function\n",
    "    def visualization_ent(self,protocol):\n",
    "        \"\"\"\n",
    "        Function that receives the name of a paper, and retrieves a plot with the most common entities found\n",
    "        Input:\n",
    "            Name of a white paper in the corpus\n",
    "        Output:\n",
    "            Plot\n",
    "        \"\"\"\n",
    "\n",
    "        assert protocol in self.data.corpora_test_whole.keys()\n",
    "\n",
    "        # Load the NLP object with pre-trained data\n",
    "        nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "        # Create an object of the paper chosen\n",
    "        doc1 = nlp(self.data.corpora_test_whole[protocol])\n",
    "\n",
    "        # Get all the entities gather from the documents\n",
    "        items1 = [x.label_ for x in doc1.ents]\n",
    "\n",
    "        # Get the most common entities found in the paper\n",
    "        i1 = Counter(items1).most_common(10)\n",
    "\n",
    "        # Pepare the arrays for the plot\n",
    "        x1 = []\n",
    "        y1 = []\n",
    "\n",
    "        # Iterate through the entities and add them into the array\n",
    "        for i in range(len(i1)):\n",
    "            x1.append(i1[i][0])\n",
    "            y1.append(i1[i][1])\n",
    "\n",
    "        # Create the visualization\n",
    "        plt.subplots_adjust(hspace = 0.5, top = 0.4)\n",
    "        plt.figure(figsize=(12,5))\n",
    "\n",
    "        plt.subplot(211)\n",
    "        plt.plot(x1, y1)\n",
    "        plt.title(protocol)\n",
    "        \n",
    "        return plt\n",
    "    \n",
    "    # Define the function\n",
    "    def visualize_wordvec(self,topic, protocol):\n",
    "        \"\"\"\n",
    "        Function that receives a word and the name of a paper, and returns a visualization with similar words\n",
    "        Input:\n",
    "            Specific topic\n",
    "            Name of the protocol\n",
    "            Corpora with all the texts\n",
    "            Word2Vec Model with all the Vectors\n",
    "        Output:\n",
    "            Plots\n",
    "        \"\"\"\n",
    "        assert protocol in self.data.corpora_test_whole.keys()\n",
    "\n",
    "        # Create an empty Word2Vec object where we will put the weights for the specific protocol, based on the whole corpus\n",
    "        model_test = Word2Vec()\n",
    "\n",
    "        # Prepare the array with the tokens\n",
    "        tokens = []\n",
    "\n",
    "        # Tokenize white paper\n",
    "        tokens.append(word_tokenize(self.data.corpora_test_whole[protocol]))\n",
    "\n",
    "        # Iterate through all the words in the specific white paper, and add the weights into the new model\n",
    "        for word in tokens[0]:\n",
    "            if word in self.model.wv.vocab:\n",
    "                model_test.wv[word] = self.model.wv[word]\n",
    "\n",
    "        # Get most similar word, based on the whole corpus\n",
    "        l = model_test.wv.most_similar([topic])\n",
    "\n",
    "        # Prepare the arrays for the visualizations\n",
    "        x = []\n",
    "        y = []\n",
    "\n",
    "        # Iterate through the similar words\n",
    "        for i in range(len(l)):\n",
    "            x.append(l[i][0])\n",
    "            y.append(l[i][1])\n",
    "\n",
    "        # Prepare Visualization\n",
    "        #grid = plt.GridSpec(2, 3, wspace=0.4, hspace=0.3)\n",
    "        #plt.subplot(grid[0, 0])   \n",
    "        plt.bar(x, y)\n",
    "        plt.xticks(rotation=90)\n",
    "        \n",
    "        return plt\n",
    "        \n",
    "    def visualize_pca(self,protocol):\n",
    "        assert protocol in self.data.corpora_test_whole.keys()\n",
    "\n",
    "\n",
    "        # Prepare the array with the tokens\n",
    "        tokens = []\n",
    "\n",
    "        # Iterate through all the papers and get all the different words within the papers\n",
    "        for k in self.data.corpora_test_whole.keys():\n",
    "            tokens.append(word_tokenize(self.data.corpora_test_whole[k]))\n",
    "\n",
    "        # Iterate through the the protocols to get the position of the protocol\n",
    "        n = 0    \n",
    "        for i in enumerate(self.data.corpora_test_whole.items()):\n",
    "            if i[1][0] == protocol:\n",
    "                n = i[0]\n",
    "            else: continue\n",
    "                \n",
    "                \n",
    "        # Model with only words in the exact protocol\n",
    "        model2 = Word2Vec(tokens[n], size =100, window=5, min_count=1, workers=2)\n",
    "\n",
    "        # Get the similar words within the exact protocol\n",
    "        X = model2[model2.wv.vocab]\n",
    "        pca = PCA(n_components=2)\n",
    "        result = pca.fit_transform(X)\n",
    "\n",
    "        # Prepare Visualization\n",
    "        #plt.subplot(grid[1, 0])\n",
    "        plt.figure(figsize=(10,10))\n",
    "\n",
    "        plt.scatter(result[:,0], result[:,1], s=100)\n",
    "        words = list(model2.wv.vocab)\n",
    "\n",
    "        for i, word in enumerate(tokens[n][:]):\n",
    "            try:\n",
    "                plt.annotate(word, xy=(result[i,0], result[i,1]), size = 10)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModeling():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \n",
    "        #Combining them and putting into pandas dataframe\n",
    "        test_combined = {key:[combine_text(value)] for (key, value) in self.data.corpora_test_sent.items()}\n",
    "        \n",
    "        self.test_df = pd.DataFrame.from_dict(test_combined).transpose()\n",
    "        self.test_df.columns = ['whitepapers']\n",
    "        self.test_df = self.test_df.sort_index()\n",
    "        self.test_df.head()\n",
    "        \n",
    "        #creating the list and tokenizing each word in the test corpus\n",
    "        testdata = test_df.whitepapers.values.tolist()\n",
    "        testdata_words = list(sent_to_words(testdata))\n",
    "\n",
    "        self.test_data_ready = process_words(testdata_words)\n",
    "\n",
    "        # Apply the Topic Model into unseen documents\n",
    "        self.corpus_lda_test = [id2word.doc2bow(text) for text in test_data_ready]\n",
    "\n",
    "    # Function that gets the Dominant Topic in a Document\n",
    "    def format_topics_sentences(self):\n",
    "        \n",
    "        # Init output\n",
    "        sent_topics_df = pd.DataFrame()\n",
    "\n",
    "        # Get main topic in each document\n",
    "        for i, row_list in enumerate(self.model[self.corpus_lda_test]):\n",
    "            row = row_list[0] if self.model.per_word_topics else row_list            \n",
    "            # print(row)\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = self.model.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "        # Add original text to the end of the output\n",
    "        contents = pd.Series(self.test_data_ready)\n",
    "        sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "        \n",
    "        return(sent_topics_df)\n",
    "\n",
    "    # Function that gets all the topics per document\n",
    "    def topics_per_document(self, model, protocol, df):\n",
    "        \n",
    "        index = self.test_df.reset_index().loc[self.test_df.index == protocol].index[0]\n",
    "        corpus_sel = [self.corpus_lda_test[index]]\n",
    "        \n",
    "        dominant_topics = []\n",
    "        topic_percentages = []\n",
    "        \n",
    "        for i, corp in enumerate(corpus_sel):\n",
    "            topic_percs, wordid_topics, wordid_phivalues = self.model[corp]\n",
    "            dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
    "            dominant_topics.append((i, dominant_topic))\n",
    "            topic_percentages.append(topic_percs)\n",
    "        \n",
    "        return(dominant_topics, topic_percentages)\n",
    "\n",
    "\n",
    "    def get_lda_model(self):\n",
    "        \n",
    "        self.model = LdaModel.load(\"model_lda_topicmodeling.model\")\n",
    "        \n",
    "    def visualize_topics(self,protocol):\n",
    "        \n",
    "        unseen_doc = corpus_lda_test[0]\n",
    "        # get topic probability distribution for a document\n",
    "        vector = lda_gensim[unseen_doc] \n",
    "\n",
    "        # Format the topics from the unseen data into the LDA Model\n",
    "        df_topic_sents_keywords = self.format_topics_sentences()\n",
    "\n",
    "        # Format\n",
    "        df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "        df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "        # Use the function to get the percentages of topics in the document\n",
    "        _, topic_percentages = self.topics_per_document(protocol=protocol)            \n",
    "\n",
    "        # Total Topic Distribution by actual weight\n",
    "        topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
    "        df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
    "\n",
    "        # Top 3 Keywords for each Topic\n",
    "        topic_top3words = [(i, topic) for i, topics in lda_gensim.show_topics(formatted=False) \n",
    "                                         for j, (topic, wt) in enumerate(topics) if j < 3]\n",
    "\n",
    "        df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
    "        df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "        df_top3words.reset_index(level=0,inplace=True)\n",
    "\n",
    "        # Plot\n",
    "        #fig, ax = plt.subplots(1, 1, figsize=(15, 6), dpi=150, sharey=True)\n",
    "\n",
    "        # Topic Distribution by Topic Weights\n",
    "        plt.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\n",
    "        plt.set_xticks(range(6))\n",
    "        tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n",
    "        plt.xaxis.set_major_formatter(tick_formatter)\n",
    "        plt.set_title('Number of Documents by Topic Weightage', fontdict=dict(size=10))\n",
    "\n",
    "        return plt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassification():\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.df_test_1 = pd.DataFrame.from_dict(self.data.corpora_test_whole,orient=\"index\").reset_index()\n",
    "\n",
    "        self.df_test_1.columns = [\"Protocol\", \"Text\"]\n",
    "        \n",
    "        # Define a Data Frame to put all the corpora_sent into a Pandas dataframe\n",
    "        self.df_test_2 = pd.DataFrame(columns=[\"Text\",\"Protocol\"])\n",
    "        \n",
    "        # Iterate over each white paper, and on each sentence, and add it to the Dataframe\n",
    "        i=0\n",
    "        for k, v in enumerate(self.data.corpora_test_sent):\n",
    "            for sent in self.data.corpora_test_sent[v]:\n",
    "                self.df_test_2.loc[i, \"Text\"] = sent\n",
    "                self.df_test_2.loc[i, \"Protocol\"] = v\n",
    "                i += 1\n",
    "        \n",
    "    def get_log_model(self):\n",
    "        # Read the csv and load it on a DataFrame\n",
    "        df = pd.read_csv(TRAIN_CSV, sep=';', encoding=\"utf-8-sig\")\n",
    "        \n",
    "        # Lower all the words\n",
    "        df.Label = df.Label.apply(lambda x : x.lower())\n",
    "\n",
    "        # Define the labels and also the Train dataset\n",
    "        labels = df.groupby(by='Label').count().sort_values(by='Label').reset_index()['Label']\n",
    "\n",
    "        X_train = df[\"Text\"]\n",
    "        y_train = df[\"Label\"]\n",
    "\n",
    "        # We need to add another category, in case there is no topic mentioned on the sentence\n",
    "        X_train[len(X_train)+1] = \"\"\n",
    "        y_train[len(y_train)+1] = \"none\"\n",
    "\n",
    "        # We train a really simple Logistic Regression\n",
    "        self.logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "                       ])\n",
    "        \n",
    "        self.logreg.fit(X_train, y_train)\n",
    "        \n",
    "    def get_nn_model(self):\n",
    "        \n",
    "        # We load our Neural Network from previous model weights\n",
    "        self.model = load_model(\"model_neuralnetwork_classification.h5\")\n",
    "        \n",
    "    def visualize_log(self,protocol):\n",
    "        # First, we define our test dataframe\n",
    "        X_test = df_test_2[\"Text\"]\n",
    "\n",
    "        # Then, for the second model, we predict it\n",
    "        predict_logreg = self.logreg.predict(X_test)\n",
    "\n",
    "        # Add into the Dataframe\n",
    "        self.df_test_2[\"Model_2\"] = predict_logreg\n",
    "\n",
    "        protocol_test = self.df_test_2[self.df_test_2.Protocol == protocol].groupby(by=\"Model_2\").count().sort_values(by=\"Text\", ascending=False)\n",
    "        protocol_test['percentage'] = protocol_test['Text']/protocol_test['Text'].sum()\n",
    "\n",
    "        protocol_test = protocol_test.drop(\"none\", axis=0).reset_index()\n",
    "\n",
    "        # PLOT\n",
    "        plt.bar(protocol_test.Model_2, protocol_test.percentage)\n",
    "        plt.title(protocol) \n",
    "        plt.xlabel(\"Train Protocols\")\n",
    "        plt.ylabel(\"% Likelihood\")\n",
    "        \n",
    "        return plt\n",
    "\n",
    "\n",
    "    def visualize_nn(self,protocol):\n",
    "        # We define the max of words that our Tokenizer will have\n",
    "        max_words = 15000\n",
    "        tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "\n",
    "        # First, we define our test dataframe\n",
    "        X_test = self.df_test_1[\"Text\"]\n",
    "\n",
    "        # For the first model, we need to tokenize the text\n",
    "        X_test = tokenize.texts_to_matrix(X_test)\n",
    "        \n",
    "        # Predict\n",
    "        predict_nn = self.model.predict(X_test)\n",
    "\n",
    "        # Add into the DataFrame\n",
    "        self.df_test_1[\"Model_1\"] = predict_nn.tolist()\n",
    "        \n",
    "        # Generate the Labels, with the name of each White Paper\n",
    "        labels = list(self.data.corpora_sent.keys())\n",
    "        \n",
    "        pd.options.display.float_format = '{}'.format\n",
    "        \n",
    "        for index, row in self.df_test_1.iterrows():\n",
    "            if row[\"Protocol\"] == protocol:\n",
    "                frame = pd.DataFrame()\n",
    "                frame[\"weights\"] = row[\"Model_1\"]\n",
    "                frame[\"label\"] = labels\n",
    "                frame = frame.sort_values(by=\"weights\", ascending=False)\n",
    "                frame = frame.head(5)\n",
    "                \n",
    "                # PLOT\n",
    "                plt.bar(frame.label, frame.weights)\n",
    "                plt.title(row[\"Protocol\"]) \n",
    "                plt.xlabel(\"Train Protocols\")\n",
    "                plt.ylabel(\"% Likelihood\")\n",
    "        \n",
    "        return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
